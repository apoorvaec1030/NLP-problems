{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Import twitter data\n",
    "2. Split train, test\n",
    "3. Process and clean the tweets\n",
    "4. Calculate word freq per label\n",
    "5. Calculate log prior of the tweet\n",
    "6. Calculate loglikelihood of each word in the tweet\n",
    "7. Calculate tweet sentiment = logprior+loglikelihood\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from os import getcwd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=f'{getcwd}/../tmp/'\n",
    "nltk.data.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets=twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_positive_tweets),'+',len(all_negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = all_positive_tweets[:4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_pos + train_neg\n",
    "train_y = np.append(np.ones(len(train_pos)),np.zeros(len(train_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_pos + test_neg\n",
    "test_y = np.append(np.ones(len(test_pos)),np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweet = 'RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    '''\n",
    "    -remove hashtag\n",
    "    -remove retweet symbol eg. \"RT\"\n",
    "    -remove stock market ticker symbols eg. $GE\n",
    "    -remove hyperlinks\n",
    "    -remove emoji\n",
    "    -tokenize\n",
    "    -remove stopwords\n",
    "    -remove punctuations\n",
    "    -stemming\n",
    "    '''\n",
    "    tweet = re.sub(r'#','',tweet) #hashtag sign rm\n",
    "    tweet = re.sub(r'\\$\\w*','',tweet) #ticker symbol rm\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+','',tweet) #hyperlinks\n",
    "    tweet = re.sub(r'^RT[\\s]+','',tweet) #retweet symbol rm\n",
    "\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    tweet = re.sub(emoj, '', tweet)\n",
    "    \n",
    "    tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tweet_tok = tknzr.tokenize(tweet)\n",
    "    \n",
    "    stem = PorterStemmer()\n",
    "    stopword = stopwords.words('english')\n",
    "    \n",
    "    clean_tweets = []\n",
    "    for word in tweet_tok:\n",
    "        if (word not in stopword and word not in string.punctuation):\n",
    "            words = stem.stem(word)\n",
    "            clean_tweets.append(words)\n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_tweet(sample_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word - label frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweet(tweets,ys):\n",
    "    '''\n",
    "    output - (word,y):freq\n",
    "    '''\n",
    "    result={}\n",
    "    \n",
    "    for twits,label in zip(tweets,ys):\n",
    "        twts=process_tweet(twits)\n",
    "     \n",
    "        for word in twts:\n",
    "           \n",
    "            pair=(word,label)\n",
    "\n",
    "            if pair in result:\n",
    "                result[pair]+=1\n",
    "            else:\n",
    "                result[pair]=1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_tweet(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweet(tweets,ys)\n",
    "#sorted(freqs.items(),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs,train_x,train_y):\n",
    "    \n",
    "    '''\n",
    "    logprior for each document/tweet\n",
    "    D_pos = tot pos documents/tweet\n",
    "    D_neg = tot neg documents\n",
    "    D = tot documents\n",
    "    logprior = log(p(D_pos/D)/p(D_neg/D))\n",
    "    \n",
    "    loglikelihood for each word in the document\n",
    "    p(W_pos)=(freq_w_pos+1)/(N_pos+V)\n",
    "    p(W_neg)=(freq_w_neg+1)/(N_neg+V)\n",
    "    loglikelihood = log(p(W_pos)/p(W_neg))\n",
    "    freq_w_pos = freq of word as label 1\n",
    "    freq_w_neg = freq of word as label 0\n",
    "    N_pos = tot num of words as label 1\n",
    "    N_neg = tot num of words as label 0\n",
    "    V = tot num of unique words in all the documents\n",
    "    '''\n",
    "    \n",
    "    loglikelihood={}\n",
    "    \n",
    "    D_pos = len(list(filter(lambda x:x==1, train_y)))\n",
    "    D_neg = len(list(filter(lambda x:x==0 , train_y)))\n",
    "    D = len(train_y)\n",
    "    \n",
    "    P_D_pos = D_pos/D\n",
    "    P_D_neg = D_neg/D\n",
    "    \n",
    "    logprior = np.log(P_D_pos)-np.log(P_D_neg)\n",
    "    \n",
    "    vocab = set(map(lambda x:x[0], freqs.keys()))\n",
    "    V = len(vocab)\n",
    "    \n",
    "    N_pos = np.sum([val for key, val in freqs.items() if key[1]==1])\n",
    "    N_neg = np.sum([val for key, val in freqs.items() if key[1]==0])\n",
    "    \n",
    "    for word in vocab:\n",
    "        freq_w_pos = freqs.get((word,1),0)\n",
    "        freq_w_neg = freqs.get((word,0),0)\n",
    "        \n",
    "        P_w_pos = (freq_w_pos+1)/((N_pos)+V)\n",
    "        P_w_neg = (freq_w_neg+1)/((N_neg)+V)\n",
    "        \n",
    "        loglikelihood[word]=np.log(P_w_pos/P_w_neg)\n",
    "        \n",
    "    \n",
    "    return logprior , loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior,loglikelihood = train_naive_bayes(freqs,train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(tweet,logprior,loglikelihood):\n",
    "    \n",
    "    tweet =  process_tweet(tweet)\n",
    "    \n",
    "    p=0\n",
    "    p+=logprior\n",
    "    \n",
    "    for word in tweet:\n",
    "        if word in loglikelihood:\n",
    "            p+=loglikelihood.get(word,0)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6338810936509134"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet='She cry.'\n",
    "predict_naive_bayes(tweet,logprior,loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_x,test_y,logprior,loglikelihood,predict_naive_bayes=predict_naive_bayes):\n",
    "    \n",
    "    accuracy=0\n",
    "    y_hat=[]\n",
    "    for predict,test in zip(test_x,test_y):\n",
    "        \n",
    "        if predict_naive_bayes(predict,logprior,loglikelihood)>0:\n",
    "            log=1\n",
    "        else:\n",
    "            log=0\n",
    "        y_hat.append(log)\n",
    "\n",
    "    error = np.mean([abs(hat-y) for hat,y in zip(y_hat,test_y)])\n",
    "    \n",
    "    accuracy = 1-error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_x,test_y,logprior,loglikelihood,predict_naive_bayes=predict_naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3237261653470738"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet='you are bad'\n",
    "predict_naive_bayes(tweet,logprior,loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
