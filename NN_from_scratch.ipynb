{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwjxVChaj5exh1A1Vp3x7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvaec1030/NLP-problems/blob/main/NN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assume a basic neural network structure - 1 dense layer\n",
        "1. Embedded Input layer\n",
        "2. Relu activated hidden layer\n",
        "3. Softmax output layer\n",
        "\n",
        "Functions:\n",
        "V-vocab size, m-batch size\n",
        "1. Initialize the weights W1,W2,b1,b2\n",
        "2. Forward prop\n",
        "3. Cross-entropy cost function\n",
        "4. Softmax\n",
        "5. Backward prop\n",
        "6. Gradient descent"
      ],
      "metadata": {
        "id": "UgRsDh6dBv9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initialize weights"
      ],
      "metadata": {
        "id": "8PisWT6WDSgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iocZqUgFBpUs"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(V,m,N):\n",
        "\n",
        "  W1 = np.random.randint(1,1000,size=(V,N))\n",
        "  W2 = np.random.randint(1,1000,size=(V,N))\n",
        "  b1 = np.random.randint(1,1000,size=(N,m))\n",
        "  b2 = np.random.randint(1,1000,size=(V,m))\n",
        "\n",
        "  return W1,W2,b1,b2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  return np.exp(z)/np.sum(np.exp(z),axis=0)"
      ],
      "metadata": {
        "id": "efvIdeSwLXEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(W1,W2,b1,b2,X,softmax=softmax):\n",
        "\n",
        "  z1 = np.dot(W1.T,X)+b1\n",
        "  h = np.maximum(z1) #relu\n",
        "\n",
        "  z2 = np.dot(W2.T,h)+b2\n",
        "  y_hat = softmax(z2)\n",
        "\n",
        "  return y_hat, h"
      ],
      "metadata": {
        "id": "3a0bwue_LLs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binarycrossentropycost(y,y_hat,m):\n",
        "\n",
        "  return (-1/m)*np.sum(np.dot(y,np.log(y_hat)) + np.dot((1-y),np.log(1-y_hat)))"
      ],
      "metadata": {
        "id": "jSbVDA2AObfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(y,y_hat,W1,W2,b1,b2,m,h,X):\n",
        "    # Compute z1 as \"W1â‹…x + b1\"\n",
        "    z1 = np.dot(W1, x) + b1\n",
        "\n",
        "    ### START CODE HERE (Replace instanes of 'None' with your code) ###\n",
        "\n",
        "    # Compute l1 as W2^T (Yhat - Y)\n",
        "    l1 = np.dot(W2.T,(yhat-y))\n",
        "\n",
        "    # if z1 < 0, then l1 = 0\n",
        "    # otherwise l1 = l1\n",
        "    # (this is already implemented for you)\n",
        "\n",
        "    l1[z1 < 0] = 0 # use \"l1\" to compute gradients below\n",
        "\n",
        "    # compute the gradient for W1\n",
        "    grad_W1 = (1/batch_size)*np.dot(l1,x.T)\n",
        "\n",
        "    # Compute gradient of W2\n",
        "    grad_W2 = (1/batch_size)*np.dot(yhat - y, h.T)\n",
        "\n",
        "    one=np.zeros(batch_size)+1\n",
        "    # compute gradient for b1\n",
        "    grad_b1 = (1/batch_size)*np.dot(l1,one.T)\n",
        "    grad_b1 = np.expand_dims(grad_b1, axis=1)\n",
        "\n",
        "    # compute gradient for b2\n",
        "    grad_b2 = (1/batch_size)*np.dot(yhat - y,one.T)\n",
        "    grad_b2 = np.expand_dims(grad_b2, axis=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Lrl1T2ePhFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(W1,W2,b1,b2,gradw1,gradw2,gradb1,gradb2,alpha):\n",
        "\n",
        "  W1 = W1 - alpha*gradw1\n",
        "  W2 = W2 - alpha*gradw2\n",
        "  b1 = b1 - alpha*gradb1\n",
        "  b2 = b2 - alpha*gradb2\n"
      ],
      "metadata": {
        "id": "7C5-P8GbR6Vh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}